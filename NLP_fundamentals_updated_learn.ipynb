{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthiharaj/python_fundamental/blob/main/NLP_fundamentals_updated_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**NLP**"
      ],
      "metadata": {
        "id": "TMe0aIKshqaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tokenization**\n",
        "**What is Tokenization?\n",
        "Tokenization is the process of breaking down text into smaller units called tokens. Tokens can be words, punctuation marks, or subwords, depending on the tokenizer.**\n",
        "\n"
      ],
      "metadata": {
        "id": "fUq9fDTlfNYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''LLM i token = 4 characters  ex busy, text, sacrifice\n",
        "busy 4 token\n",
        "sacrifi'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-l7L3KfqiURd",
        "outputId": "b51866f5-c9ac-441f-f1c3-5fe9bee8583e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LLM i token = 4 characters  ex busy, text, sacrifice\\nbusy 4 token \\nsacrifi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "adr1u97geu57"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Wld8JfC7gGGU"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"Myself Karthiha, I am studing AI ML at minnion for 60 k at jan26, i am from india\""
      ],
      "metadata": {
        "id": "N5f5lvZLf7fX"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sending input to model\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "eJlQx1NAeu2e"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print tokens\n",
        "print(\"Tokens:\")\n",
        "for token in doc:\n",
        "    print(token.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFXTX7pHeuzR",
        "outputId": "32611521-e4ff-4ca7-d708-6c0c11e455a6"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "Myself\n",
            "Karthiha\n",
            ",\n",
            "I\n",
            "am\n",
            "studing\n",
            "AI\n",
            "ML\n",
            "at\n",
            "minnion\n",
            "for\n",
            "60\n",
            "k\n",
            "at\n",
            "jan26\n",
            ",\n",
            "i\n",
            "am\n",
            "from\n",
            "india\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l-00ghqMhZpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**POS Tagging**\n",
        "\n",
        "** What is POS Tagging?\n",
        "Part-of-speech tagging assigns grammatical tags (like noun, verb, adjective) to each token in a sentence.\n",
        "\n",
        "Why is it Important?\n",
        "It helps understand the grammatical structure of a sentence.\n",
        "Useful in applications like grammar checking, parsing, and syntactic analysis **"
      ],
      "metadata": {
        "id": "wxUXqCVaiK9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print tokens with their POS tags\n",
        "print(\"Tokens with POS tags:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6egyUMOib5s",
        "outputId": "fa64c1b7-c9fc-4352-df33-2205e1b30a9e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens with POS tags:\n",
            "Myself: PROPN (NNP)\n",
            "Karthiha: PROPN (NNP)\n",
            ",: PUNCT (,)\n",
            "I: PRON (PRP)\n",
            "am: AUX (VBP)\n",
            "studing: VERB (VBG)\n",
            "AI: PROPN (NNP)\n",
            "ML: PROPN (NNP)\n",
            "at: ADP (IN)\n",
            "minnion: NOUN (NN)\n",
            "for: ADP (IN)\n",
            "60: NUM (CD)\n",
            "k: NOUN (NN)\n",
            "at: ADP (IN)\n",
            "jan26: PROPN (NNP)\n",
            ",: PUNCT (,)\n",
            "i: PRON (PRP)\n",
            "am: AUX (VBP)\n",
            "from: ADP (IN)\n",
            "india: PROPN (NNP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Named Entity Recognition (NER)\n",
        "What is NER?\n",
        "Named Entity Recognition identifies and classifies entities in text into predefined categories like:**"
      ],
      "metadata": {
        "id": "CycKrO3JjbGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print named entities\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsnZ-_IfjhI2",
        "outputId": "d8e4e8fa-98ad-41c3-fb88-54de071eb412"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "Myself Karthiha: PERSON (People, including fictional)\n",
            "AI ML: ORG (Companies, agencies, institutions, etc.)\n",
            "60: CARDINAL (Numerals that do not fall under another type)\n",
            "india: GPE (Countries, cities, states)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available Named Entities in spaCy:\")\n",
        "for label in nlp.get_pipe(\"ner\").labels:\n",
        "    explanation = spacy.explain(label)\n",
        "    print(f\"{label}: {explanation if explanation else 'No description available'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNwKuA2DjvKA",
        "outputId": "db3b7bce-e7be-4537-db22-bd20c37e52a3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Named Entities in spaCy:\n",
            "CARDINAL: Numerals that do not fall under another type\n",
            "DATE: Absolute or relative dates or periods\n",
            "EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
            "FAC: Buildings, airports, highways, bridges, etc.\n",
            "GPE: Countries, cities, states\n",
            "LANGUAGE: Any named language\n",
            "LAW: Named documents made into laws.\n",
            "LOC: Non-GPE locations, mountain ranges, bodies of water\n",
            "MONEY: Monetary values, including unit\n",
            "NORP: Nationalities or religious or political groups\n",
            "ORDINAL: \"first\", \"second\", etc.\n",
            "ORG: Companies, agencies, institutions, etc.\n",
            "PERCENT: Percentage, including \"%\"\n",
            "PERSON: People, including fictional\n",
            "PRODUCT: Objects, vehicles, foods, etc. (not services)\n",
            "QUANTITY: Measurements, as of weight or distance\n",
            "TIME: Times smaller than a day\n",
            "WORK_OF_ART: Titles of books, songs, etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAIlMYVGkUzd"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Python package for natural language processing"
      ],
      "metadata": {
        "id": "V8zNYDfkE2dm"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''n Natural Language Toolkit (NLTK), \"stem\" refers to the process of reducing a word to its root form by removing suffixes,\n",
        "and \"NLTK stem .proper\" means using the NLTK library to apply a stemming algorithm to a word, essentially extracting the base form of that word,\n",
        " like changing \"running\" to \"run\" or \"studies\" to \"study\" by removing the endings;\n",
        "  this is a common technique used in text analysis to group related words togethe\n",
        "'''"
      ],
      "metadata": {
        "id": "N1W5FZngmHsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING, LEMMATIZATIONS is used to convert the wod into its root form"
      ],
      "metadata": {
        "id": "cBjy2E0XmlhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK is like ChatGPT\n",
        "nltk is opensource\n",
        "chtGPT is payable **bold text**"
      ],
      "metadata": {
        "id": "_EqCm8GMohmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *"
      ],
      "metadata": {
        "id": "3vgBMmUDE2aF"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Tokenization is the process of converting text into individual words or tokens,\n",
        "while lemmatization is the process of converting words to their base or root forms.'''\n",
        "# import these modules\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "8Sz6A50xFPPn"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# choose some words to be stemmed\n",
        "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\", \"driver\", \"trainer\", \"calculator\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "WeKLwEaPFHeX"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in words:\n",
        "\tprint(w, \" : \", ps.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiQsutXZFHa8",
        "outputId": "01bd5cab-ea29-47b0-e1fd-a6eff6d650d6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programmer  :  programm\n",
            "programming  :  program\n",
            "programmers  :  programm\n",
            "driver  :  driver\n",
            "trainer  :  trainer\n",
            "calculator  :  calcul\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "own_words = [\"slicing\", \"baking\", \"smiling\", \"denoting\", \"cursing\", \"swmming\", \"dances\"]"
      ],
      "metadata": {
        "id": "icjfUbNpFy4q"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in own_words:\n",
        "\tprint(w, \" : \", ps.stem(w)) #THIS IS STEMMING"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LcXHkUsF-7p",
        "outputId": "08ea238d-e492-4a06-9247-4aca7ffeae60"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slicing  :  slice\n",
            "baking  :  bake\n",
            "smiling  :  smile\n",
            "denoting  :  denot\n",
            "cursing  :  curs\n",
            "swmming  :  swmming\n",
            "dances  :  danc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqj4TIzeFqaX",
        "outputId": "8a933f02-cbf1-4f0f-fa96-b3e7d5adb1ed"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lz = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "uTJCZ21ephf8"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"rocks :\", lz.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lz.lemmatize(\"corpora\"))\n",
        "print(\"denoting :\", lz.lemmatize(\"denoting\"))\n",
        "print(\"dances :\", lz.lemmatize(\"dances\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg-vpE53FSOB",
        "outputId": "ed539c67-493b-45bb-846c-9d9b1a17936b"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "denoting : denoting\n",
            "dances : dance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "junck_list = [\"ing\", \"er\", \"es\"]\n",
        "input_text =\"swimming\""
      ],
      "metadata": {
        "id": "hN7ts-0rFmFL"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK** nltk.org"
      ],
      "metadata": {
        "id": "EX_wdgZto3EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3k6GrRgSvTeO",
        "outputId": "14c62e30-bae8-4ec3-fbd4-6cc69a6435bc"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-03daf1e0f6cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    }
  ]
}